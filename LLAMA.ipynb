{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOoysxxR2JQdLFLNQaOdYxP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdalrahmenyousifMohamed/LLM/blob/main/LLAMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAqgg3ITbtoM",
        "outputId": "3c2e6a2d-50d9-44e7-91bd-61d3404eecf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DocQA'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 30 (delta 9), reused 14 (delta 2), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (30/30), 8.41 KiB | 2.10 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/afaqueumer/DocQA.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/DocQA'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZZRsfy6byQJ",
        "outputId": "5601ba55-0302-40f1-dcb6-5d5748a8186e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DocQA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwVJXi21b5B4",
        "outputId": "0aca608d-ae16-43b1-bfd7-7c8d29f4209a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\tPipfile  README.md  requirements.txt  run_app.bat  setup_env.bat  temp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pipenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_UIgZI5b6C3",
        "outputId": "4c979c3f-68a4-49b4-82b3-ec3f85c4e11d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pipenv\n",
            "  Downloading pipenv-2023.11.15-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pipenv) (2023.11.17)\n",
            "Requirement already satisfied: setuptools>=67 in /usr/local/lib/python3.10/dist-packages (from pipenv) (67.7.2)\n",
            "Collecting virtualenv>=20.24.2 (from pipenv)\n",
            "  Downloading virtualenv-20.25.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting distlib<1,>=0.3.7 (from virtualenv>=20.24.2->pipenv)\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.24.2->pipenv) (3.13.1)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.24.2->pipenv) (4.1.0)\n",
            "Installing collected packages: distlib, virtualenv, pipenv\n",
            "Successfully installed distlib-0.3.8 pipenv-2023.11.15 virtualenv-20.25.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pipenv install --python /usr/bin/python3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES66E7d8cZlz",
        "outputId": "fa5c8630-d11c-476a-a9ef-e4fd70226608"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mCreating a virtualenv for this project...\u001b[0m\n",
            "Pipfile: \u001b[33m\u001b[1m/content/Pipfile\u001b[0m\n",
            "\u001b[1mUsing\u001b[0m \u001b[33m\u001b[1m/usr/bin/python3\u001b[0m \u001b[32m(3.10.12)\u001b[0m \u001b[1mto create virtualenv...\u001b[0m\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Creating virtual environment...\u001b[36mcreated virtual environment CPython3.10.12.final.0-64 in 428ms\n",
            "  creator CPython3Posix(dest=/root/.local/share/virtualenvs/content-cQIIIOO2, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: pip==23.3.1, setuptools==69.0.2, wheel==0.42.0\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n",
            "\u001b[0m\n",
            "✔ Successfully created virtual environment!\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Creating virtual environment...\n",
            "\u001b[1A\u001b[2K\u001b[32mVirtualenv location: /root/.local/share/virtualenvs/content-cQIIIOO2\u001b[0m\n",
            "\u001b[1mCreating a Pipfile for this project\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "\u001b[1mPipfile.lock not found, creating\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "Locking\u001b[0m \u001b[33m[packages]\u001b[0m dependencies...\u001b[0m\n",
            "Locking\u001b[0m \u001b[33m[dev-packages]\u001b[0m dependencies...\u001b[0m\n",
            "\u001b[1mUpdated Pipfile.lock (8e6547742479a83bb67555446f723053c2894d670dcc89b1a6e1e07cab885a1b)!\u001b[0m\n",
            "\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1m885a1b\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
            "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pipenv shell | ls | exti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl_DJ4MOcf0X",
        "outputId": "bddfd880-6b70-440f-e22b-72fe6fab5dc5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching subshell in virtual environment...\n",
            "\u001b[?2004h\u001b[01;34m/content\u001b[00m#  . /root/.local/share/virtualenvs/content-cQIIIOO2/bin/activate\r\n",
            "\u001b[?2004l\r\u001b[?2004h(content) \u001b[01;34m/content\u001b[00m# \n",
            "Aborted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! CMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJPRf_X-co85",
        "outputId": "4dd47dda-abfb-4774-f10c-c391c1ce2873"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.25)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade huggingface_hub"
      ],
      "metadata": {
        "id": "ScUDP4nIRHhg",
        "outputId": "2ee0f868-b0c4-43fb-df3e-d74e7491e54d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface_hub\n",
            "  Using cached huggingface_hub-0.20.1-py3-none-any.whl (330 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.11.17)\n",
            "Installing collected packages: huggingface_hub\n",
            "Successfully installed huggingface_hub-0.20.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "ews = hf_hub_download(repo_id=\"TheBloke/CodeLlama-13B-GGUF\", filename=\"codellama-13b.Q4_0.gguf\")\n",
        "\n",
        "# hf_hub_download(repo_id=\"google/fleurs\", filename=\"fleurs.py\", repo_type=\"dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVLV-DTOgghL",
        "outputId": "95e6362d-fd38-41a2-e125-c2bfdc539966"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "J8dRfGfuhoFD",
        "outputId": "cda7fabf-002e-4a7c-9091-4ec74de3594d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.cache/huggingface/hub/models--TheBloke--CodeLlama-13B-GGUF/snapshots/dc64dd5ad4c82364e7b0fe119d544b131dfdb8bc/codellama-13b.Q4_0.gguf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf /root/.cache/huggingface/hub/models--TheBloke--LLaMa-7B-GGML\n",
        "# !ls"
      ],
      "metadata": {
        "id": "34Wia1KojSKb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(model_path='/root/.cache/huggingface/hub/models--TheBloke--CodeLlama-13B-GGUF/snapshots/dc64dd5ad4c82364e7b0fe119d544b131dfdb8bc/codellama-13b.Q4_0.gguf')\n",
        "result = llm('who is the president of egypt ?')\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHQIt9pac3jF",
        "outputId": "d478753b-6760-4a1a-c7c7-0e98d68d0eb7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-8b576878-e6b9-4d55-bd75-6e71872fbef4',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1703281665,\n",
              " 'model': '/root/.cache/huggingface/hub/models--TheBloke--CodeLlama-13B-GGUF/snapshots/dc64dd5ad4c82364e7b0fe119d544b131dfdb8bc/codellama-13b.Q4_0.gguf',\n",
              " 'choices': [{'text': '\\na. anwar al-sadat b. abdel hamid',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'length'}],\n",
              " 'usage': {'prompt_tokens': 9, 'completion_tokens': 16, 'total_tokens': 25}}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm('who is the  president of Egypt no3',top_p=0.2,temperature=0.3)\n",
        "result['choices'][0]['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "4fSPLDmcfeBU",
        "outputId": "a4efb37e-f1ce-4355-a182-19b88c79af77"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEgyptian President Hosni Mubarak has been in power'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install langchain\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter"
      ],
      "metadata": {
        "id": "Y-BSBEqdqJtS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TextLoader?"
      ],
      "metadata": {
        "id": "eKZIjCn9iOCz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Loader = TextLoader('/content/Abdalrahmen.txt')\n",
        "docs = Loader.load()"
      ],
      "metadata": {
        "id": "1o6ElOvFTYe2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os.path import split\n",
        "text_splitter = CharacterTextSplitter(chunk_size=20,chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "6GCHYL3HYhvL",
        "outputId": "7d1e8467-dd04-4469-d318-c20b9b61e010",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 80, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 61, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 69, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 70, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 67, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 426, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 249, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 158, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 451, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 78, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2236, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 75, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 276, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 429, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 247, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 465, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 250, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 232, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 548, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 74, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 326, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 380, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 335, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 78, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 372, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 348, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 74, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 74, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 60, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 75, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 329, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 510, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 373, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 194, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 226, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 434, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 74, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 229, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 75, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 69, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 57, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 21, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 71, which is longer than the specified 20\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 535, which is longer than the specified 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "id": "XdU39GIZZBt6",
        "outputId": "cd17a405-1058-4bc0-b6d4-7d30d3f4dd60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts"
      ],
      "metadata": {
        "id": "q3a-jm3wZ6Iz",
        "outputId": "950f7ab9-fd7e-4ba0-bccd-3a631cc4bff4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='1\\nAbdalrahmen                     Yousef                Machine         Learning', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='abdalrahmenyousif54@gmail.com              +201097904132', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='Egypt , Mansoura                                        Exempted', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='LinkedIn                                                      Git', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='abdalrahmen', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='Profile', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content=\"• Software Engineer with a focus on artificial  intelligence and machine learning. \\n   experience proficient in creating, deploying, and managing end-to-end AI projects. I \\n   specialize in Agile methodologies and Software Development Life Cycle, ensuring \\n   efficient and timely delivery. My expertise extends to Generative AI, where I've led \\n   innovative projects pushing the boundaries of traditional problem-solving\", metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='• Adept at delivering impactful results and contributing to team success in dynamic, \\n   collaborative environments. Excited to leverage my expertise in creating and deploying \\n   cutting-edge AI solutions to drive innovation in your organization', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='•My dedication to the field of Machine Learning and Artificial Intelligence, along with my \\n   problem-solving skills and knowledge of related technologies', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='• I also undertook internships that provided hands-on experience in implementing machine \\n   learning algorithms. In one particular internship, I contributed to the development of a \\n   recommendation system that significantly improved user engagement. This experience \\n   allowed me to apply my theoretical knowledge in a professional setting and exposed me to \\n   the intricacies of deploying machine learning models in a production environment.', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='2\\n                                                                      Skills', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='Generative AI                 AWS                                Google Cloud                  Tensorflow  & \\n(Llama,                           Amazon Bedrok ,            Platform                           Keras\\nOpenAI,Amazon             Lambda Function ,          Deploy machine              Build varies models \\nBedrock)                         SageMaker , s3              leaning  model in             from scratch + read \\nFine-Tuned models        buckets , IAM                  production                       books master \\nand train them in a         permissions                     environment                    tensorflow\\nspecific use cases\\n                                       terraform   &                    PySpark  &                      Yolov8\\nFASTAPI & FLASK        MLflow                             PostgreSQL Data           Fine-tune model in \\n& Docker CI/CD                                                     analysis                           Car Plate \\nGitHub                            C++                                                                         Characters \\n                                       How to build neural         pytorch                            Recognition with \\npython                            network  and apply          Build deep learning         YOLO\\nused advanced               memory efficiency           models with full \\ntechniques for                                                        control  over                    C#\\nbuilding models              Additional   Skills             training process              Created Desktop \\n                                       1.CyperSecurity                                                      apps\\nTime Series                    (OSCP , Web                  Data Analysis\\ncollaborated to                Pentesting ,                    Excel , PowerBI ,  \\nmake financial                Network)                          Building \\nanalysis                          2. Flutter                          DashBoards\\n                                       development apps \\n                                       & Engagment API & \\n                                       Backend\\n                                       Laravel', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='Projects', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='LBP, Low Back Pain                                                                                          December 2022 –\\nLow back pain |Lumber spain detection by values provided  by                                August 2023\\nsensors is placed in back of patient or MRI data', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='3\\nDrowness detection   while  driving,  Implementing  a system to                               June 2021 –\\ndetect drowsiness during driving involves the use of various                              November  2021\\ntechnologies and sensors.\\nFacial Recognition , Eye-Tracking Technology, Steering \\nBehavior Analysis , Machine Learning Models , Wearable \\nDevices , Alert Systems , Infrared Sensors , Integration with \\nVehicle Systems', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='TwitGenius                                                                                                              August 2023 –\\na groundbreaking startup  at the intersection  of AI and social                                October 2023\\nmedia', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='Reinforcement  Learning,  RL\\nMy experience with Reinforcement Learning has not only \\nenhanced my technical skills but also provided  me with a deep \\nunderstanding of how intelligent   agents can learn to make \\noptimal decisions in dynamic  and uncertain environments.  I am \\npassionate about leveraging RL techniques to solve real-world \\nproblems and drive innovations  in AI-driven applications \\n(Autonomous Driving &G ame Playing & Automated Trading \\nStrategies )', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='AskY,                                                                                                       July 2023 – August 2023\\nhttps://github.com/abdalrahmenyousifMohamed/LLM/tree/2185\\n147ea69a9c684582aeb46aff439988766102/ClassGPT\\nAsk Your Files', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='Car Plate Characters Recognition, \\nCar Plate Characters Recognition with YOLO\\nLed a comprehensive  exploration   to develop  a robust \\nmachine learning   model  for Arabic  letter  and number \\nrecognition  on  Egyptian  car plates', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='4\\n   • Explored Convolutional Neural Networks (CNNs) for \\n   sequential  data handling, experimenting  with  diverse CNN \\n   encoders and sequential decoders. Employed  the PyTorch \\n   Lightning  framework for streamlined  and organized code, \\n   while leveraging the  new PyTorch v2 transformers module for \\n   efficient  data augmentation.  Achieved noteworthy \\n   advancements in both  model accuracy and code structure, \\n   contributing  to  the cutting-edge landscape of Arabic \\n   character recognition  in the context of Egyptian  car plates', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='Awards', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='Devfest Mansoura  Hackathon,  GDG                                                             22nd October 2023\\nParticipated in Devfest Mansoura Hackathon 2023 as a \\ncompetitor  from Sector X and successfully nished as a First \\nPlace , achieved by GDG Delta to provide our community with \\nnew solutions for community  problems', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='Kaggle, Open Problems – Single-Cell Perturbations\\nChampioned breakthroughs in  Open Problems – Single-Cell \\nPerturbations, achieving remarkable  success with low error \\nrate predictions on various perturbations.  Spearheaded \\ninitiatives that  significantly  advanced the field and showcased \\na commitment  to delivering high-impact  results in complex \\nproblem-solving  scenarios', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='lablab,  lablab.ai\\nIndependently participated  in  the TruEra Challenge, \\nshowcasing my proficiency by building  LLM Applications with \\nGoogle Cloud Vertex AI on lablab. Successfully conceptualized \\nand created a powerful app, demonstrating  not only technical \\nprowess but also a proactive approach to innovation  and \\nproblem-solving', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='5\\n                                                                Certificates', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='Forge Data Analysis       (Completed a simulation focused on advising a client on\\n   customer targeting with  the Data, Analytics & Modelling team - Assessed data quality\\n   and completeness in preparation for analysis - Analysed data to target high-value\\n   customers based on demographics and attributes -Developed dashboards to\\n   communicate findings with visuals)', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='Building  LLM-Powered  Applications        (I am adept at architecting and building\\n   Language Model (LLM)-powered applications that redefine user experiences. Proficient in\\n   leveraging cutting-edge techniques such as Transformers and attention mechanisms, I\\n   create intelligent systems for natural language understanding  and generation.)', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='Amazon Bedrock  (How to use with Prompt Engineering and Fine-tune LLM)', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='Languages', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='English                                         Arabic\\nIELTS', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='Courses', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='Amazon Bedrock,  AWS                                                                                    November 2023 –\\nHow to use LLMS and Fine-tune and Planning a Generative AI                          December 2023\\nproject and how to use Prompt Engineering                                                       Mansoura, Egypt', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='Data Analysis, Forage                                                                                           October 2023 –\\nCompleted a simulation  focused on advising a client on                                    November  2023\\ncustomer targeting\\n with the Data, Analytics & Modelling team\\n Assessed data quality and completeness in preparation for \\nanalysis\\n  Analysed data to target high-value customers based on \\ndemographics and\\n attributes\\n  Developed dashboards to communicate findings with  visuals', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='6\\nLLm apps, Wandb                                                                                            September  2023 –\\nHow to Build LLM apps and deploy in production                                                    October 2023\\n                                                                                                                     Mansoura, Mansoura', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='SQL, DataCamp                                                                                  February 2022 – April 2022\\nlearned how to use SQL and PostgreSQL to make full analysis \\nfor dataset', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='TimeSeries Analysis,  DataScience365                                                  March 2022 – June 2023\\nHow to deal with Time series data and going all the way of \\npreprocessing , cleaning,transforming,use Statistics etc.', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content=\"Machine Learning,  Microsoft Courses                                                                               Online\\n apply these classic techniques to data from many areas of the \\nworld. Each lesson includes pre- and post-lesson quizzes, \\nwritten instructions  to complete  the lesson, a solution, an \\nassignment, and more. Our project-based pedagogy allows you \\nto learn while building,  a proven way for new skills to 'stick'.\", metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='Education', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='Software  Engineering,  Mansoura University                                         July 2019 – August 2023\\nBachelor of computer sciences and informations  in major                           Manoura, Mansoura\\nSoftware Engineering', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='Publications', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='Books', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='• Introduction to natural language processing using deep learning', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='• Introduction to deep learning starting  with PyTorch', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='•Master TensorFlow', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='7\\n                                                           References', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='NTI Tech Solutions, Egypt ,Mansoura, \\nLed cross-functional teams in gathering requirements and successfully delivered \\ntransformative AI solutions, fostering seamless collaboration and ensuring alignment with \\nproject objectives, \\nPioneered research initiatives on state-of-the-art AI technologies, translating findings into \\npractical applications for impactful integration into real-world projects. Demonstrated a \\ncommitment to excellence in team dynamics and the application of cutting-edge \\ntechnologies to achieve project success', metadata={'source': '/content/Abdalrahmen.txt'}),\n",
              " Document(page_content='Machine Learning python | Mansoura University, \\nproven track record of implementing computer vision algorithms for Sound and Viedo \\nProcessing, pivotal to an autonomous vehicle detection project. Spearheaded groundbreaking \\nresearch on state-of-the-art AI technologies, resulting in their successful application to real-\\nworld projects, \\nCollaborated with data scientists and engineers to preprocess and analyze large datasets for \\nmachine learning projects and Contributed to the improvement of existing machine learning \\nmodels through insightful analysis and experimentation', metadata={'source': '/content/Abdalrahmen.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "id": "AE55I9SFZ8FW",
        "outputId": "e2e7c63c-e5b1-4dd7-b8b8-47c260067e85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0]"
      ],
      "metadata": {
        "id": "3HJ3ONKta-6F",
        "outputId": "0ef2a356-a20d-4347-e6ab-22710b4ffff5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='1\\nAbdalrahmen                     Yousef                Machine         Learning', metadata={'source': '/content/Abdalrahmen.txt'})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import LlamaCppEmbeddings\n",
        "embeddings = LlamaCppEmbeddings(model_path=ews)"
      ],
      "metadata": {
        "id": "78aBAgKMbEOt",
        "outputId": "c0021a0d-25f4-4b08-9857-4ccad2dc758c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_texts = []\n",
        "\n",
        "for i in range(len(texts)):\n",
        "  _texts.append(texts[i].page_content)\n",
        "\n",
        "\n",
        "embedded_texts = embeddings.embed_documents(_texts)\n",
        "len(embedded_texts) , len(embedded_texts[0])"
      ],
      "metadata": {
        "id": "0iquvwjPcxxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_texts[0][:4]"
      ],
      "metadata": {
        "id": "dr4KHSgLdb-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'What skills did Batman had?'\n",
        "embedded_query = embeddings.embed_query(query)"
      ],
      "metadata": {
        "id": "vvrtTfF_dqRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "Gom5DY2Md5HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "puzgTnBLgGKR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}